{"cells":[{"cell_type":"code","source":["#fileName = dbutils.widgets.get('fileName')\nfileName = 'Product.csv'\nfileNameWithoutExt = fileName.split('.')[0]\nprint(fileNameWithoutExt)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58fbf49e-f6a3-4be2-a21f-087946ac0595","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\n# fill with required details of resources\n\nsqlDbName = 'project2db'     # DB name\ndbUserName = 'sqladminuser' # DB username\ndbPasswordKey = 'dbpassword'  # DB password - Vault Secret's\nstorageAccountSASToken = 'storageAccountSASTkn' # Storageaccount SAS tocken - Vault's Secret\nlandingFileName = fileName\ndatabricksScopeName = 'databricksToken01' # Azure Databrick scope credential\ndbServer = 'project2dbserver'\ndbServerPortNumber = '1433'\nstorageContainer = 'project2'\nstorageAccount= 'stgeaccntdlake'\nlandingMountPoint = '/mnt' # Mounting location\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6259a115-ab8d-46f2-9a36-7bda72962b56","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Mounting the storage location\nif not any(mount.mountPoint == landingMountPoint for mount in dbutils.fs.mounts()):\n    dbutils.fs.mount( source = 'wasbs://{}@{}.blob.core.windows.net'.format(storageContainer, storageAccount), mount_point= landingMountPoint, extra_configs ={'fs.azure.sas.{}.{}.blob.core.windows.net'.format(storageContainer,storageAccount):dbutils.secrets.get(scope = databricksScopeName, key= storageAccountSASToken)})\n    print('Successfully mounted the storage account')\nelse:\n    print('Storage account is already mounted')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1791d71e-246d-4fb9-ab59-9283fc2a0267","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Connect to Azure SQL DB\ndbPassword = dbutils.secrets.get(scope = databricksScopeName, key= dbPasswordKey)\nserverurl = 'jdbc:sqlserver://{}.database.windows.net:{};database={};user={};'.format(dbServer, dbServerPortNumber,sqlDbName,dbUserName)\nconnectionProperties = {\n    'password':dbPassword,\n    'driver':'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n}\ndataframe_DB = spark.read.jdbc(url = serverurl, table = 'dbo.FileDetailsFormat', properties= connectionProperties)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be56aba3-aee9-4567-8590-49cbf7dc6eb9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataframe_File = spark.read.csv('/mnt/landing/'+fileName, inferSchema=True, header=True)\n\n# Rule 1 :- Checking for duplicate rows\nerrorFlag=False\nerrorMessage = ''\ntotalcount = dataframe_File.count()\nprint(totalcount)\ndistinctCount = dataframe_File.distinct().count()\nprint(distinctCount)\nif distinctCount !=totalcount:\n    errorFlag = True\n    errorMessage = 'Duplication Found. Rule 1 Failed'\nprint(errorMessage)\n    \n# Rule 2 : - Checking the column format\ndataframe_Filtered = dataframe_DB.filter(dataframe_DB.FileName==fileNameWithoutExt).select('ColumnName', 'ColumnDateFormat' )\nrows = dataframe_Filtered.collect()\nfor r in rows:\n    colName = r[0]\n    colFormat =r[1]\n    formatCount =dataframe_File.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n    if formatCount != totalcount:\n        errorFlag = True\n        errorMessage = errorMessage +' DateFormate is incorrect for {} '.format(colName)\n    else:\n        print('All rows are good for ', colName)\nprint(errorMessage)\n\nif errorFlag:\n    dbutils.fs.mv('/mnt/landing/'+fileName,'/mountloc/rejected/'+fileName )\n    dbutils.notebook.exit('{\"errorFlag\": \"true\", \"errorMessage\":\"'+errorMessage +'\"}')\nelse:\n    dataframe_File.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ProductDelta\")\n    dbutils.fs.mv('/mnt/landing/'+fileName,'/mountloc/staging/'+fileName )\n    dbutils.notebook.exit('{\"errorFlag\": \"false\", \"errorMessage\":\"No error\"}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"638c358e-b149-4675-ad1c-33376b3d542e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM ProductDelta"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"ea5b7ac7-3c74-4c5e-8860-deb23e7bfa45","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"AP_notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":484233464795592,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2943585277873781}},"nbformat":4,"nbformat_minor":0}
